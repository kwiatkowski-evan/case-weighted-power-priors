# ###############################################################################
#
# # Save HR to innermost
# innermost[idx, c("HR_com_cauchy",
#                  "HR_com_gamma"
#                  # ,"HR_com_no_ext",
#                  # "HR_com_full_ext"
#                  )] <-
#   as.numeric(unlist(com_results[com_results$prior %in% c("cauchy",
#                                                          "gamma"
#                                                          # ,"no_ext",
#                                                          # "full_ext"
#                                                          ), "mean_HR_trt_cc"]))
#
#
# # Save power to innermost
# innermost[idx, c("power_com_cauchy",
#                  "power_com_gamma"
#                  # ,"power_com_no_ext",
#                  # "power_com_full_ext"
#                  )] <-
#   as.numeric(unlist(com_results[com_results$prior %in% c("cauchy",
#                                                          "gamma"
#                                                          # ,"no_ext",
#                                                          # "full_ext"
#                                                          ), "reject"]))
###############################################################################
###############################################################################
###############################################################################
#  For each idx_scale and idx_sample, summarize data from idx_n repeats
innermost <- data.frame(innermost)
innermost$scale   <- scale_list[idx_scale]  # scale
innermost$bias_a0 <- (innermost$HR_a0 - exp(mu2["treat"])) # BIAS
# Compute MSE, HR, credible interval width, and power for each analysis method
for (i in 1:length(nList)){
innermost[[paste0("MSE_", nList[i])]]      <-
(innermost[[paste0("HR_", nList[i])]] - exp(mu2["treat"])) ^ 2
innermost[[paste0("CI_width_", nList[i])]] <-
(innermost[[paste0("upper_", nList[i])]] - innermost[[paste0("lower_", nList[i])]])
innermost[[paste0("CI_", nList[i])]]       <-
(innermost[[paste0("lower_", nList[i])]] < exp(mu2["treat"]) &
exp(mu2["treat"]) < innermost[[paste0("upper_", nList[i])]])
innermost[[paste0("power_", nList[i])]]    <-
((innermost[[paste0("lower_", nList[i])]] < 1 & innermost[[paste0("upper_", nList[i])]] < 1)
# | (innermost[[paste0("lower_", nList[i])]] > 1 & innermost[[paste0("upper_", nList[i])]] > 1)
)
}
outermost[idx_outer, ] <- colMeans(innermost)
}
SD_a0 <- sd(outermost[, "HR_a0"])
names(SD_a0) <- "SD_a0"
T1E_print <- T1E
names(T1E_print) <- "T1E"
write.csv(c(colMeans(outermost), SD_a0, T1E_print), paste0("../output/",  formatC(idx_scale, width=4, flag="0"), ".csv"))
# }
idx_scale
idx_scale <- 792
idx_scale
load(file = 'args_model.RData') # loads all model information include prior parameters AND SETS SEED
if (Sys.getenv("USER") %in% c("kwiatkoe", "evankwiatkowski")) {
setwd("/Users/kwiatkoe/Documents/GitHub/hybrid-methods/pprior-a0j/large-sample-share/code")
library(ggplot2)
library(survival)
library(ks)
library(foreach)
library(doParallel)
library(survminer)
library(MASS)
library(ecdata)
library(hesim)
library(rjags)
library(psborrow)
idx_scale <- 792
} else {                                    # longleaf
library(pracma, lib.loc = "../rpkgs/")
library(gnorm,  lib.loc = "../rpkgs/")
args <- commandArgs(trailingOnly = TRUE)  # sequence from batch file
idx_scale  <- as.numeric(args[1]);
}
set.seed(as.integer(idx_scale*1E4))  #  05-19-2020
idx_scale
# # register cores from doParallel package
# registerDoParallel(detectCores())
# getDoParWorkers()
#
# # Do not load "nseg2.Rdata" unless there is no access to Flatiron
# # load("nseg2.Rdata")
#
# ### BEGIN FIXED PARAMETERS IF LOADING .RDATA
# n.seg      <- 2 # number of points used to determine baseline hazard, including zero and infinity
# # e.g. n.seg = 2 is constant baseline hazard, n.seg = 3 has two components
# cov.names  <- c("age", "sex2") # set covariate names
# scale_list <- rep(c(rep(seq(-log(2), log(2), length = 9), each = 7), 0), 156) # covariate effect on log scale for external control subjects with missing ecog
# T1E        <- 1 # (if T1E = 1 then treatment effect set to zero)
# idx_samp   <- 1 # number of simulations (i.e. RCT and external data generated)
# idx_n      <- 1 # repetitions per design (i.e. number of imputations per exposure time interval in external controls)
# idx_a0_n   <- 1 # repetitions per a0 (i.e. number of compatibility scores computed per exposure time interval)
# trt_n      <- 425 # number of treated subjects to use
# ctrl_n     <- 213 # number of control subjects to use
# h_n        <- 212 # number of external (historical) subjects to use
# ### END SET PARAMETERS ###
#
# ################################################################################
# ################################################################################
# ################################################################################
#
# # Declare formulas for time-to-events
# formula2 <- formula(paste("nu ~ treat +", paste(cov.names, collapse=" + "), " + interval + offset(log(expo)) - 1"))
# formula1 <- formula(paste("nu ~ treat +", paste(cov.names, collapse=" + "), " + offset(log(expo))"))
#
# # https://pages.github.roche.com/RWDScodeshare/ecdata/#credentials
# oak_fi <- pins::pin_get("nct02008227_fi", board = "local")
#
# # Recode dataset
# oak_fi <- oak_fi %>%
#   dplyr::mutate(os_days = ifelse(os_days <= 0, 1E-4, os_days)) %>%    # Reset os_days if less than 0 %>%
#   dplyr::mutate(sex2 = ifelse(sex == "M", 1, 0)) %>%                  # Numeric variable for sex
#   dplyr::mutate(treat = ifelse(arm_type == "Experimental", 1, 0)) %>% # Numeric variable for treatment
#   dplyr::mutate(external = ifelse(source_type == "External", 1, 0))   # Numeric variable for external
# oak_fi$ecog <- as.numeric(oak_fi$ecog)
# oak_fi$t   <- oak_fi$os_days
# oak_fi$nu  <- oak_fi$os_status
# X.all <- oak_fi[, c("treat", cov.names, "ecog", "treat", "external")]
#
# # Determine number of cut-points for baseline hazard based on equal number of events per interval
# if (n.seg >= 3){
#   inner.t  <- quantile(as.vector(oak_fi$os_days)[as.vector(oak_fi$os_status) == 1],
#                        probs = seq(0, 1, length = n.seg))[2:(n.seg - 1)]
#   cut.time <- c(0, inner.t)
#   diff     <- cut.time[-1] - cut.time[-length(cut.time)]
# } else {
#   cut.time <- 0
# }
# interval.names <- paste0("interval", seq(1:(length(cut.time))))
#
# # Fit models for events and censoring based on proportional hazards model among RCT subjects
# # Covariate effects for events are mu2, and for censoring are theta2
# sim.split  <- survSplit(oak_fi, cut = cut.time, end = "t", start = "t0", event = "nu", episode = "interval")
# sim.split$interval <- factor(sim.split$interval - 1)
# sim.split$expo     <- sim.split$t - sim.split$t0
# if (n.seg >= 3){
#   sim.poisson <- glm(formula2,
#                      family = poisson(link = "log"),
#                      data = sim.split[sim.split$source_type == "RCT", ])
# } else {
#   sim.poisson <- glm(formula1,
#                      family = poisson(link = "log"),
#                      data = sim.split[sim.split$source_type == "RCT", ])
#   names(sim.poisson$coefficients)[names(sim.poisson$coefficients) == "(Intercept)"] <- "interval1"
# }
# mu2 <- sim.poisson$coefficients
#
# if (n.seg >= 3){
#   sim.poisson <- glm(1 - nu ~ interval + offset(log(expo)) - 1,
#                      family = poisson(link = "log"),
#                      data = sim.split[sim.split$source_type == "RCT", ])
# } else {
#   sim.poisson <- glm(1 - nu ~ offset(log(expo)),
#                      family = poisson(link = "log"),
#                      data = sim.split[sim.split$source_type == "RCT", ])
#   names(sim.poisson$coefficients)[names(sim.poisson$coefficients) == "(Intercept)"] <- "interval1"
# }
# theta2 <- sim.poisson$coefficients
#
# if (n.seg >= 3){
#   mu2["interval1"] <- -6.797565718
#   mu2["interval2"] <- -6.797565718
#   mu2["treat"] <- -0.311859408
#   mu2["age"] <- 0.008968345
#   mu2["sex2"] <- 0.229839210
#   theta2["interval1"] <- -6.956003
#   theta2["interval2"] <- -6.956003
# }
#
# # Set column names for stored data
# a <- c("HR", "MSE", "lower", "upper", "CI_width", "CI", "power", cov.names)
# b <- c("a0", "0", "0.25", "0.5", "0.75", "1", "f", "iptw", "cox")
# c <- c("com_cauchy", "com_gamma", "com_no_ext", "com_full_ext")
# nList <- b
# ppList <- nList[1:6]
# names <- c(paste(expand.grid(a,b)$Var1, expand.grid(a,b)$Var2, sep = "_"),
#            paste(expand.grid(a,c)$Var1, expand.grid(a,c)$Var2, sep = "_"),
#            "SE_a0", "a0_logHR", "bias_a0", "scale", "mean_a0",
#            "a0_wt_ecog0", "a0_wt_ecog1", "a0_wt_ecogNA",
#            c(paste0("a0_wt_int", 1:(n.seg - 1))),
#            c(paste("a0_wts", expand.grid(c(0,1,2),1:(n.seg - 1))$Var1, expand.grid(c(0,1,2),1:(n.seg - 1))$Var2, sep = "_")))
# outer_table <- matrix(NA, nrow = length(scale_list), ncol = length(names))
# colnames(outer_table) <- names
# foreach(idx_scale = 1:length(scale_list)) %dopar% {
# for(idx_scale in 1:length(scale_list)){
# for(idx_scale in 1:1){
outermost <- matrix(NA, nrow = idx_samp, ncol = length(names))
colnames(outermost) <- names
formula2 <- formula(paste("nu ~ treat +", paste(cov.names, collapse=" + "), " + interval + offset(log(expo)) - 1"))
formula1 <- formula(paste("nu ~ treat +", paste(cov.names, collapse=" + "), " + offset(log(expo))"))
formula2_no_trt <- formula(paste("nu ~ ", paste(cov.names, collapse=" + "), " + interval + offset(log(expo)) - 1"))
formula1_no_trt <- formula(paste("nu ~ ", paste(cov.names, collapse=" + "), " + offset(log(expo))"))
# (if T1E = 1 then treatment effect set to zero)
if (T1E == 1){
mu2["treat"] <- 0
}
# Reformat data to create rate matricies used to generate piecewise exponential data
# ratemat for events
# ratemat2 for censoring
X.long <- X.all[rep(seq_len(nrow(X.all)), each = length(cut.time)), ]
for (j in 1:length(cut.time)){
X.long[, paste0("interval", j)] <- rep(c(rep(0, j - 1), 1, rep(0, length(cut.time) - j)), nrow(X.all))
}
X.long <- as.matrix(X.long)
ratemat <- matrix(X.long[, c("treat", cov.names, interval.names)] %*%
mu2[c("treat", cov.names, interval.names)],
ncol = length(cut.time), byrow = T)
ratemat2 <- matrix(rep(theta2, nrow(X.all)), ncol = length(cut.time), byrow = T)
# number of simulations (i.e. RCT and external data generated)
for(idx_outer in 1:idx_samp){
trt.idx  <- sample(which(X.all$treat == 1), trt_n)
ctrl.idx <- sample(which(X.all$treat == 0 & X.all$external == 0), ctrl_n)
h.idx    <- sample(which(X.all$external == 1), h_n)
h.ecog.idx <- is.na(X.all[h.idx, "ecog" ]) # length h.idx
# Simulated data
t.sim <- rpwexp(n = trt_n + ctrl_n + h_n,
rate = as.matrix(exp(ratemat[c(trt.idx, ctrl.idx, h.idx), ] +
matrix(c(rep(0, trt_n + ctrl_n), h.ecog.idx)*scale_list[idx_scale], ncol = 1) %*%
matrix(1, ncol = length(cut.time), nrow = 1))),
time = cut.time)
c.sim <- rpwexp(n = trt_n + ctrl_n + h_n,
rate = as.matrix(exp(ratemat2[c(trt.idx, ctrl.idx, h.idx), ])),
time = cut.time)
y.sim <- pmin(t.sim, c.sim)
nu.sim <- as.numeric(y.sim == t.sim) # equals 1 if observed
# Simulated design matrix
sim.data           <- data.frame(rbind(cbind(t = y.sim, nu = nu.sim,
X.all[c(trt.idx, ctrl.idx, h.idx),
c("treat", "external", cov.names, "ecog")])))
# Add patient ID to sim.data
sim.data$id <- seq.int(nrow(sim.data))
# Add iptw
ps_formula <- formula(paste("1 - external ~", paste(cov.names, collapse=" + ")))
ps_fits <- glm(ps_formula,
family = binomial(),
data = sim.data)
ps <- predict(ps_fits, type = "response")
iptw_att <- ps / (1 - ps)
sim.data <- sim.data %>% dplyr::mutate(iptw_att = ifelse(external == 1, iptw_att, 1))
# Reformat data using survSplit()
sim.split          <- survSplit(sim.data, cut = cut.time, end = "t", start = "t0",
event = "nu", episode = "interval")
sim.split$interval <- factor(sim.split$interval - 1)
sim.split$expo     <- sim.split$t - sim.split$t0
# Fit model for historical events
if (n.seg >= 3){
sim.poisson <- glm(formula2_no_trt,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 1, ])
} else {
sim.poisson <- glm(formula1_no_trt,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 1, ])
names(sim.poisson$coefficients)[names(sim.poisson$coefficients) == "(Intercept)"] <- "interval1"
}
lambda <- mvrnorm(1, sim.poisson$coefficients[c(cov.names, interval.names)],
vcov(sim.poisson)[c(cov.names, interval.names),
c(cov.names, interval.names)])
# Fit model for historical censoring
if (n.seg >= 3){
sim.poisson <- glm(1 - nu ~ interval + offset(log(expo)) - 1,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 1, ])
} else {
sim.poisson <- glm(1 - nu ~ offset(log(expo)),
family = poisson(link = "log"),
sim.split[sim.split$external == 1, ])
names(sim.poisson$coefficients)[names(sim.poisson$coefficients) == "(Intercept)"] <- "interval1"
}
theta <- mvrnorm(1, sim.poisson$coefficients[interval.names],
vcov(sim.poisson)[interval.names, interval.names])
innermost  <- matrix(NA, nrow = idx_n, ncol = length(names))
colnames(innermost) <- names
a0.outer <- matrix(NA, nrow = nrow(sim.split[sim.split$external == 1, ]), ncol = idx_n)
# repetitions per design (i.e. number of imputations per exposure time interval in external controls)
for(idx in 1:idx_n){
# Add imputation for multiple intervals in historical controls
if (n.seg >= 3){
for (i in which(sim.split$external == 1)){
for(j in 1:length(inner.t)){
if (sim.split$nu[i] == 0 & sim.split$interval[i] == j & sim.split$expo[i] == diff[j]){
temp.t <- rexp(1, rate = exp(as.matrix(sim.split[i, c(cov.names)]) %*%
as.matrix(lambda[c(cov.names)]) +
lambda[paste0("interval", j)]))
temp.c  <- rexp(1, rate = exp(theta[paste0("interval", j)]))
temp.y  <- pmin(temp.t, temp.c)
temp.nu <- as.numeric(temp.y == temp.t)
sim.split$expo[i] <- diff[j] + temp.y
sim.split$nu[i]   <- temp.nu
}
}
}
}
# Fit model for RCT events
if (n.seg >= 3){
sim.poisson.lambda <- glm(formula2,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 0, ])
} else {
sim.poisson.lambda <- glm(formula1,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 0, ])
names(sim.poisson.lambda$coefficients)[names(sim.poisson.lambda$coefficients) == "(Intercept)"] <- "interval1"
}
# Fit model for historical censoring
if (n.seg >= 3){
sim.poisson.theta   <- glm(1 - nu ~ interval + offset(log(expo)) - 1,
family = poisson(link = "log"),
data = sim.split[sim.split$external == 1, ])
} else {
sim.poisson.theta   <- glm(1 - nu ~ offset(log(expo)),
family = poisson(link = "log"),
data = sim.split[sim.split$external == 1, ])
names(sim.poisson.theta$coefficients)[names(sim.poisson.theta$coefficients) == "(Intercept)"] <- "interval1"
}
# subset to historical only
sim.split.h <- sim.split[sim.split$external == 1, ]
box.p.log <- matrix(NA, ncol = idx_a0_n, nrow = nrow(sim.split.h))
# repetitions per a0 (i.e. number of compatibility scores computed per exposure time interval)
for (idx_a0 in 1:idx_a0_n){
lambda <- mvrnorm(1, sim.poisson.lambda$coefficients[c(cov.names, interval.names)],
vcov(sim.poisson.lambda)[c(cov.names, interval.names),
c(cov.names, interval.names)])
theta <- mvrnorm(1, sim.poisson.theta$coefficients[interval.names],
vcov(sim.poisson.theta)[interval.names, interval.names])
# debugging code - prevent theta from being positive
for (i in 1:length(theta)){
if (theta[i] > 0) theta[i] <- min(theta)
}
# generate y.h.pred incorporating variability in estimating lambda
# and access compatibility, use interval + expo
for(i in 1:nrow(sim.split.h)){
j <- c(sim.split.h[i, "interval"])
y.h.pred <- rexp(1E4, rate = exp(as.matrix(sim.split.h[i, c(cov.names)]) %*%
as.matrix(lambda[c(cov.names)]) +
lambda[paste0("interval", j)]) +
exp(theta[paste0("interval", j)]))
fhat_outer         <- kde(log(y.h.pred))
fhat_predict_outer <- predict(fhat_outer, x = log(y.h.pred))
box.p.log[i, idx_a0] <- mean(fhat_predict_outer <= predict(fhat_outer, x = log(sim.split.h$expo[i])))
}
}
## Compute case-specific weights
a0 <- rowMeans(box.p.log)
a0.outer[, idx_n] <- a0
sim.split.h$a0 <- a0
# Compute average weights a0 overall and by ecog score
innermost[idx, "mean_a0"] <- mean(a0)
innermost[idx, "a0_wt_ecog0"] <- mean(sim.split.h[sim.split.h[,"ecog"] %in% 0, "a0"])
innermost[idx, "a0_wt_ecog1"] <- mean(sim.split.h[sim.split.h[,"ecog"] %in% 1, "a0"])
innermost[idx, "a0_wt_ecogNA"] <- mean(sim.split.h[is.na(sim.split.h[,"ecog"]), "a0"])
# Compute average weights a0 by interval, and by ecog and interval combination
for (i1 in c(0, 1)){
for (j1 in 1:(n.seg - 1)){
innermost[idx, paste0("a0_wts_", i1, "_", j1)] <-
mean(sim.split.h[sim.split.h[,"ecog"] %in% i1 & sim.split.h[,"interval"] == j1, "a0"])
}
}
for (j1 in 1:(n.seg - 1)){
innermost[idx, paste0("a0_wts_", 2, "_", j1)] <-
mean(sim.split.h[is.na(sim.split.h[,"ecog"]) & sim.split.h[,"interval"] == j1, "a0"])
innermost[idx, paste0("a0_wt_int", j1)] <- mean(sim.split.h[sim.split.h[,"interval"] == j1, "a0"])}
# Final analysis using fixed power prior weights & adaptive weights
for (i in 1:length(ppList)){
if (ppList[i] == "a0"){
wts <- c(rep(1, times = sum(sim.split$external == 0)), a0)
} else {
wts <- c(rep(1, times = sum(sim.split$external == 0)), rep(as.numeric(ppList[i]), sum(sim.split$external == 1)))
}
if (n.seg >= 3){
sim.poisson <- glm(formula2,
family = poisson(link = "log"),
data = sim.split,
weights = wts)
} else {
sim.poisson <- glm(formula1,
family = poisson(link = "log"),
data = sim.split,
weights = wts)
}
innermost[idx, paste0(c("HR_", "lower_", "upper_"), ppList[i])] <-
c(exp(coef(sim.poisson))["treat"],
exp(coef(sim.poisson)["treat"])*exp(-1.96*summary(sim.poisson)$coefficients["treat", 2]),
exp(coef(sim.poisson)["treat"])*exp(1.96*summary(sim.poisson)$coefficients["treat", 2]))
if (ppList[i] == "a0"){
innermost[idx, "a0_logHR"] <- coef(sim.poisson)["treat"]
innermost[idx, "SE_a0"] <- summary(sim.poisson)$coefficients["treat", 2]
}
}
# Final analysis using frailty model
f_formula <- formula(paste("Surv(t, nu) ~ treat + frailty(external) +",
paste(cov.names, collapse=" + ")))
coxf <- tryCatch(coxph(f_formula, data = sim.data),
error = function(e) coxph(f_formula, data = sim.data))
innermost[idx, c("HR_f", "lower_f", "upper_f", paste0(cov.names, "_f"))] <-
c(exp(coef(coxf))["treat"],
exp(coef(coxf)["treat"])*exp(-1.96*summary(coxf)$coefficients["treat", "se(coef)"]),
exp(coef(coxf)["treat"])*exp(1.96*summary(coxf)$coefficients["treat", "se(coef)"]),
summary(coxf)$coefficients[cov.names, "coef"])
# Final analysis using cox model
cox_formula <- formula(paste("Surv(t, nu) ~ treat + ", paste(cov.names, collapse=" + ")))
cox <- coxph(cox_formula, data = sim.data)
innermost[idx, c("HR_cox", "lower_cox", "upper_cox", paste0(cov.names, "_cox"))] <-
c(exp(coef(cox))["treat"],
exp(coef(cox)["treat"])*exp(-1.96*summary(cox)$coefficients["treat", "se(coef)"]),
exp(coef(cox)["treat"])*exp(1.96*summary(cox)$coefficients["treat", "se(coef)"]),
summary(cox)$coefficients[cov.names, "coef"])
# Final analysis using iptw
iptw_formula <-  formula(paste("Surv(t, nu) ~ treat + cluster(id) + ", paste(cov.names, collapse=" + ")))
iptw <- coxph(iptw_formula,
data = sim.data,
weights = iptw_att)
innermost[idx, c("HR_iptw", "lower_iptw", "upper_iptw", paste0(cov.names, "_iptw"))] <-
c(exp(coef(iptw))["treat"],
exp(coef(iptw)["treat"])*exp(-1.96*summary(iptw)$coefficients["treat", "robust se"]),
exp(coef(iptw)["treat"])*exp(1.96*summary(iptw)$coefficients["treat", "robust se"]),
summary(iptw)$coefficients[cov.names, "coef"])
}
# ###############################################################################
# # Final analysis using commensurate priors
# ###############################################################################
#
# sim.data_com <- sim.data
# sim.data_com$HR <- NA
# sim.data_com$driftHR <- NA
# oldNms <-c("driftHR", "HR", "external", "treat", "age", "sex2", "t", "nu")
# sim.data_com <- sim.data_com[, oldNms] # reorder & subset
# colNms <- c("driftHR","HR","ext","trt","cov1","cov2","time","cnsr")
# testLst <- list(rep(1:850), colNms)
# dimnames(sim.data_com) <- testLst # change names
# sim.data_com$time <- sim.data_com$time / 30.4167
# sim.data_com$cnsr <- 1 - sim.data_com$cnsr
# ###############################################################################
#
# pr1 <- set_prior(pred = "all", prior = "cauchy", r0 = 1, alpha = c(0, 0), sigma = 0.03)
# pr2 <- set_prior(pred = "all", prior = "gamma", r0 = 1,  alpha = c(0, 0)) # could cut gamma
# pr3 <- set_prior(pred = "all", prior = "no_ext", r0 = 1, alpha = 0)
# pr4 <- set_prior(pred = "all", prior = "full_ext", r0 = 1, alpha = 0)
# pr_list = c(pr1, pr2)
#
# ###############################################################################
#
# res <- run_mcmc_p(dt = list(sim.data_com),
#                 pr_list,
#                 n.chains = 2,
#                 n.adapt = 100,
#                 n.burn = 100, # could be lower
#                 n.iter = 300, # too low for tail probs
#                                # should be 10k
#                 seed = 47) # change seed for datasets
# com_results <- get_summary(res)
#
# ###############################################################################
#
# # Save HR to innermost
# innermost[idx, c("HR_com_cauchy",
#                  "HR_com_gamma"
#                  # ,"HR_com_no_ext",
#                  # "HR_com_full_ext"
#                  )] <-
#   as.numeric(unlist(com_results[com_results$prior %in% c("cauchy",
#                                                          "gamma"
#                                                          # ,"no_ext",
#                                                          # "full_ext"
#                                                          ), "mean_HR_trt_cc"]))
#
#
# # Save power to innermost
# innermost[idx, c("power_com_cauchy",
#                  "power_com_gamma"
#                  # ,"power_com_no_ext",
#                  # "power_com_full_ext"
#                  )] <-
#   as.numeric(unlist(com_results[com_results$prior %in% c("cauchy",
#                                                          "gamma"
#                                                          # ,"no_ext",
#                                                          # "full_ext"
#                                                          ), "reject"]))
###############################################################################
###############################################################################
###############################################################################
#  For each idx_scale and idx_sample, summarize data from idx_n repeats
innermost <- data.frame(innermost)
innermost$scale   <- scale_list[idx_scale]  # scale
innermost$bias_a0 <- (innermost$HR_a0 - exp(mu2["treat"])) # BIAS
# Compute MSE, HR, credible interval width, and power for each analysis method
for (i in 1:length(nList)){
innermost[[paste0("MSE_", nList[i])]]      <-
(innermost[[paste0("HR_", nList[i])]] - exp(mu2["treat"])) ^ 2
innermost[[paste0("CI_width_", nList[i])]] <-
(innermost[[paste0("upper_", nList[i])]] - innermost[[paste0("lower_", nList[i])]])
innermost[[paste0("CI_", nList[i])]]       <-
(innermost[[paste0("lower_", nList[i])]] < exp(mu2["treat"]) &
exp(mu2["treat"]) < innermost[[paste0("upper_", nList[i])]])
innermost[[paste0("power_", nList[i])]]    <-
((innermost[[paste0("lower_", nList[i])]] < 1 & innermost[[paste0("upper_", nList[i])]] < 1)
# | (innermost[[paste0("lower_", nList[i])]] > 1 & innermost[[paste0("upper_", nList[i])]] > 1)
)
}
outermost[idx_outer, ] <- colMeans(innermost)
}
SD_a0 <- sd(outermost[, "HR_a0"])
names(SD_a0) <- "SD_a0"
T1E_print <- T1E
names(T1E_print) <- "T1E"
write.csv(c(colMeans(outermost), SD_a0, T1E_print), paste0("../output/",  formatC(idx_scale, width=4, flag="0"), ".csv"))
# }
